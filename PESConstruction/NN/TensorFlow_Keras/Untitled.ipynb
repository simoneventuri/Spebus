{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import numpy\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from NNInput         import NNInput\n",
    "from LoadData        import load_data, abscissa_to_plot, load_parameters, load_parameters_PIP\n",
    "from MLP             import build_MLP_model\n",
    "from SaveData        import save_labels, save_parameters, save_parameters_NoBiases, save_parameters_PIP, save_to_plot\n",
    "from Plot            import plot_history, plot_try_set, plot_error, plot_scatter, plot_overall_error, plot_set\n",
    "from TransformOutput import InverseTransformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_optimization(NNInput):\n",
    "\n",
    "\n",
    "    ##################################################################################################################################\n",
    "    ### LOADING DATA\n",
    "    ##################################################################################################################################\n",
    "    print('\\nLoading Data ... \\n')\n",
    "\n",
    "    if (NNInput.TryNNFlg > 0):\n",
    "        datasets, datasetsTry = load_data(NNInput)\n",
    "    else:\n",
    "        datasets = load_data(NNInput)\n",
    "\n",
    "\n",
    "    # RSetTrainValid, ySetTrainValid, ySetTrainValidDiat, ySetTrainValidTriat = datasets[0]\n",
    "    # RSetTest,       ySetTest,       ySetTestDiat,       ySetTestTriat       = datasets[1]\n",
    "    # RDataOrig,      yDataOrig,      yDataDiatOrig,      yDataTriatOrig      = datasets[2]\n",
    "\n",
    "    # NNInput.NIn  = RSetTrainValid.shape[1]\n",
    "    # NNInput.NOut = ySetTrainValid.shape[1] \n",
    "    # print(('  Nb of Input:  %i')    % NNInput.NIn)\n",
    "    # print(('  Nb of Output: %i \\n') % NNInput.NOut)\n",
    "\n",
    "    # NNInput.NLayers = NNInput.NHid\n",
    "    # NNInput.NLayers.insert(0,NNInput.NIn)\n",
    "    # NNInput.NLayers.append(NNInput.NOut)\n",
    "    # print('  Network Shape: ', NNInput.NLayers, '\\n')\n",
    "\n",
    "    # NTrainValid = RSetTrainValid.shape[0]\n",
    "    # NTest       = RSetTest.shape[0]\n",
    "    # print(('  Nb of Training + Validation Examples: %i')    % NTrainValid)\n",
    "    # print(('  Nb of Test                  Examples: %i \\n') % NTest)\n",
    "\n",
    "    # NBatchTrainValid = NTrainValid // NNInput.NMiniBatch\n",
    "    # print(('  Nb of Training + Validation Batches: %i') % NBatchTrainValid)\n",
    "\n",
    "\n",
    "    RSetTrain, ySetTrain, ySetTrainDiat, ySetTrainTriat = datasets[0]\n",
    "    RSetValid, ySetValid, ySetValidDiat, ySetValidTriat = datasets[1]\n",
    "    RDataOrig, yDataOrig, yDataDiatOrig, yDataTriatOrig = datasets[2]\n",
    "\n",
    "    NNInput.NIn  = RSetTrain.shape[1]\n",
    "    #NNInput.NOut = ySetTrain.shape[1] \n",
    "    print(('  Nb of Input:  %i')    % NNInput.NIn)\n",
    "    print(('  Nb of Output: %i \\n') % 1)\n",
    "\n",
    "    NNInput.NLayers = NNInput.NHid\n",
    "    NNInput.NLayers.insert(0,NNInput.NIn)\n",
    "    NNInput.NLayers.append(NNInput.NOut)\n",
    "    print('  Network Shape: ', NNInput.NLayers, '\\n')\n",
    "\n",
    "    NTrain = RSetTrain.shape[0]\n",
    "    NValid = RSetValid.shape[0]\n",
    "    print('  Nb of Training + Validation Examples: ', NTrain + NValid, '; of which: ', NTrain, ' for Training and ', NValid, ' for Validation')\n",
    "    #NTest  = RSetTest.shape[0]\n",
    "    #print(('  Nb of Test                  Examples: %i \\n') % NTest)\n",
    "\n",
    "\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print('\\nBuilding the Model ... \\n')\n",
    "\n",
    "    model = build_MLP_model(NNInput)\n",
    "    #model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "\n",
    "    early_stop   = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=NNInput.ImpThold, patience=NNInput.NPatience, restore_best_weights=True, verbose=1)\n",
    "\n",
    "    WeightsPath  = NNInput.CheckpointFldr + '/weights.csv'\n",
    "    mc_callback  = tf.keras.callbacks.ModelCheckpoint(filepath=NNInput.CheckpointFilePath, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1)\n",
    "\n",
    "    lr_callback  = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=500, mode='auto', min_delta=1.e-6, cooldown=0, min_lr=1.e-8, verbose=1)\n",
    "\n",
    "    tb_callback  = tf.keras.callbacks.TensorBoard(log_dir=NNInput.CheckpointFldr, histogram_freq=100, batch_size=NNInput.NMiniBatch, write_graph=True, write_grads=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)\n",
    "\n",
    "    callbacksVec = [mc_callback, early_stop, tb_callback]\n",
    "\n",
    "    ### Training the NN\n",
    "    print('\\nTraining the Model ... \\n')\n",
    "    # history = model.fit(RSetTrainValid, ySetTrainValid, shuffle=True, batch_size=NNInput.NMiniBatch, epochs=NNInput.NEpoch, validation_split=NNInput.PercValid, verbose=1, callbacks=callbacksVec)\n",
    "    xTrain = RSetTrain #tf.convert_to_tensor(RSetTrain, tf.float32)\n",
    "    yTrain = ySetTrain #tf.convert_to_tensor(ySetTrain, tf.float32)\n",
    "    xValid = RSetValid #tf.convert_to_tensor(RSetValid, tf.float32)\n",
    "    yValid = ySetValid #tf.convert_to_tensor(ySetValid, tf.float32)\n",
    "    history = model.fit(xTrain, yTrain, shuffle=True, batch_size=NNInput.NMiniBatch, epochs=NNInput.NEpoch, validation_data=(xValid, yValid), verbose=1, callbacks=callbacksVec)\n",
    "\n",
    "    ### Plotting History\n",
    "    #plot_history(NNInput, history)\n",
    "\n",
    "    #ErrorTest = model.evaluate(RSetTest, ySetTest, verbose=1)\n",
    "    #print(\"TensorBoard LogDir: \", NNInput.CheckpointFldr)\n",
    "\n",
    "    \n",
    "    model.load_weights(NNInput.CheckpointFilePath)\n",
    "    jLayer = -1\n",
    "    for iLayer in [1,3,4,5]:\n",
    "        jLayer = jLayer+1\n",
    "        Params = model.get_layer(index=jLayer).get_weights()\n",
    "        PathToFldr = NNInput.PathToOutputFldr + NNInput.LayersName[iLayer] + '/'\n",
    "        if not os.path.exists(PathToFldr):\n",
    "            os.makedirs(PathToFldr)\n",
    "        PathToFile = PathToFldr + 'Weights.npz'\n",
    "        numpy.savez(PathToFile, Params[0], Params[1])\n",
    "        if (NNInput.WriteFinalFlg > 0):\n",
    "            if (jLayer==0): \n",
    "                save_parameters_PIP(PathToFldr, Params[0], Params[1])\n",
    "            else:\n",
    "                save_parameters(PathToFldr, Params[0], Params[1])\n",
    "\n",
    "\n",
    "    yPredOrig   = model.predict(RDataOrig)\n",
    "    yPredOrig   = InverseTransformation(NNInput, yPredOrig, yDataDiatOrig)\n",
    "    plot_scatter(NNInput, yPredOrig, yDataOrig)\n",
    "\n",
    "\n",
    "\n",
    "    ### Evaluating Model for a Particular Data-Set\n",
    "    if (NNInput.TryNNFlg > 0):\n",
    "        \n",
    "        i=-1\n",
    "        for Ang in NNInput.AngVector:\n",
    "            i=i+1\n",
    "            xSetTry, ySetTry, ySetTryDiat, ySetTryTriat = datasetsTry[i]\n",
    "            yPredTry = model.predict(xSetTry)\n",
    "            yPredTry = InverseTransformation(NNInput, yPredTry, ySetTryDiat)\n",
    "            ### Saving Predicted Output\n",
    "            #PathToTryLabels = NNInput.PathToOutputFldr + '/yEvaluated.csv'\n",
    "            #save_labels(PathToTryLabels, 'Generated', yPredTry)\n",
    "            PathToAbscissaToPlot = NNInput.PathToDataFldr + '/R.csv.' + str(Ang)\n",
    "            xPlot = abscissa_to_plot(PathToAbscissaToPlot)\n",
    "            #PathToTryLabels = NNInput.PathToOutputFldr + '/REBestDet.csv.' + str(Ang)\n",
    "            PathToTryLabels = NNInput.PathToOutputFldr + '/REBestAll.csv.' + str(Ang)\n",
    "            # ErrorAbs    =  ySetTry - yPredTry\n",
    "            # ErrorRel    = (ySetTry - yPredTry) / ySetTry\n",
    "            # AbsErrorAbs = abs(  ySetTry - yPredTry            )\n",
    "            # AbsErrorRel = abs( (ySetTry - yPredTry) / ySetTry )\n",
    "            save_to_plot(PathToTryLabels, 'Evaluated', numpy.concatenate((xPlot, ySetTry, yPredTry), axis=1))\n",
    "            #save_to_plot_all(PathToTryLabels, 'Evaluated', numpy.concatenate((xPlot, ySetTry, yPredTry, ErrorAbs, ErrorRel, AbsErrorAbs, AbsErrorRel), axis=1))\n",
    "            \n",
    "            # ### Plotting Results\n",
    "            # plot_try_set(NNInput, ySetTry, yPredTry)\n",
    "    \n",
    "        error = ySetTry - yPredTry\n",
    "        plot_error(NNInput, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(NNInput):\n",
    "\n",
    "    \n",
    "    ##################################################################################################################################\n",
    "    ### LOADING DATA\n",
    "    ##################################################################################################################################\n",
    "    print('\\nLoading Data ... \\n')\n",
    "\n",
    "    if (NNInput.TryNNFlg > 0):\n",
    "        datasets, datasetsTry = load_data(NNInput)\n",
    "    else:\n",
    "        datasets = load_data(NNInput)\n",
    "\n",
    "    RSetTrainValid, ySetTrainValid, ySetTrainValidDiat, ySetTrainValidTriat = datasets[0]\n",
    "    RSetTest,       ySetTest,       ySetTestDiat,       ySetTestTriat       = datasets[1]\n",
    "    RDataOrig,      yDataOrig,      yDataDiatOrig,      yDataTriatOrig      = datasets[2]\n",
    "\n",
    "\n",
    "    NNInput.NIn  = RSetTrainValid.shape[1]\n",
    "    NNInput.NOut = ySetTrainValid.shape[1] \n",
    "    print(('  Nb of Input:  %i')    % NNInput.NIn)\n",
    "    print(('  Nb of Output: %i \\n') % NNInput.NOut)\n",
    "\n",
    "    NNInput.NLayers = NNInput.NHid\n",
    "    NNInput.NLayers.insert(0,NNInput.NIn)\n",
    "    NNInput.NLayers.append(NNInput.NOut)\n",
    "    print('  Network Shape: ', NNInput.NLayers, '\\n')\n",
    "\n",
    "    NTrainValid = RSetTrainValid.shape[0]\n",
    "    NTest       = RSetTest.shape[0]\n",
    "    print(('  Nb of Training + Validation Examples: %i')    % NTrainValid)\n",
    "    print(('  Nb of Test                  Examples: %i \\n') % NTest)\n",
    "\n",
    "    NBatchTrainValid = NTrainValid // NNInput.NMiniBatch\n",
    "    print(('  Nb of Training + Validation Batches: %i') % NBatchTrainValid)\n",
    "\n",
    "\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print('\\nBuilding the Model ... \\n')\n",
    "\n",
    "    model = build_MLP_model(NNInput)\n",
    "    #model.summary()\n",
    "\n",
    "    \n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    \n",
    "    model.load_weights(NNInput.CheckpointFilePath)\n",
    "    jLayer = -1\n",
    "    for iLayer in [1,3,4,5]:\n",
    "        jLayer = jLayer+1\n",
    "        Params = model.get_layer(index=jLayer).get_weights()\n",
    "        PathToFldr = NNInput.PathToOutputFldr + NNInput.LayersName[iLayer] + '/'\n",
    "        if not os.path.exists(PathToFldr):\n",
    "            os.makedirs(PathToFldr)\n",
    "        PathToFile = PathToFldr + 'Weights.npz'\n",
    "        numpy.savez(PathToFile, Params[0], Params[1])\n",
    "        if (NNInput.WriteFinalFlg > 0):\n",
    "            if (jLayer==0): \n",
    "                save_parameters_PIP(PathToFldr, Params[0], Params[1])\n",
    "            else:\n",
    "                save_parameters(PathToFldr, Params[0], Params[1])\n",
    "\n",
    "\n",
    "    yPredOrig   = model.predict(RDataOrig)\n",
    "    yPredOrig   = InverseTransformation(NNInput, yPredOrig, yDataDiatOrig)\n",
    "    plot_scatter(NNInput, yPredOrig, yDataOrig)\n",
    "\n",
    "\n",
    "    ### Evaluating Model for a Particular Data-Set\n",
    "    if (NNInput.TryNNFlg > 0):\n",
    "        \n",
    "        i=-1\n",
    "        for Ang in NNInput.AngVector:\n",
    "            i=i+1\n",
    "            xSetTry, ySetTry, ySetTryDiat, ySetTryTriat = datasetsTry[i]\n",
    "            yPredTry = model.predict(xSetTry)\n",
    "            yPredTry = InverseTransformation(NNInput, yPredTry, ySetTryDiat)\n",
    "            ### Saving Predicted Output\n",
    "            #PathToTryLabels = NNInput.PathToOutputFldr + '/yEvaluated.csv'\n",
    "            #save_labels(PathToTryLabels, 'Generated', yPredTry)\n",
    "            PathToAbscissaToPlot = NNInput.PathToDataFldr + '/R.csv.' + str(Ang)\n",
    "            xPlot = abscissa_to_plot(PathToAbscissaToPlot)\n",
    "            #PathToTryLabels = NNInput.PathToOutputFldr + '/REBestDet.csv.' + str(Ang)\n",
    "            PathToTryLabels = NNInput.PathToOutputFldr + '/REBestAll.csv.' + str(Ang)\n",
    "            # ErrorAbs    =  ySetTry - yPredTry\n",
    "            # ErrorRel    = (ySetTry - yPredTry) / ySetTry\n",
    "            # AbsErrorAbs = abs(  ySetTry - yPredTry            )\n",
    "            # AbsErrorRel = abs( (ySetTry - yPredTry) / ySetTry )\n",
    "            save_to_plot(PathToTryLabels, 'Evaluated', numpy.concatenate((xPlot, ySetTry, yPredTry), axis=1))\n",
    "            #save_to_plot_all(PathToTryLabels, 'Evaluated', numpy.concatenate((xPlot, ySetTry, yPredTry, ErrorAbs, ErrorRel, AbsErrorAbs, AbsErrorRel), axis=1))\n",
    "            \n",
    "            # ### Plotting Results\n",
    "            # plot_try_set(NNInput, ySetTry, yPredTry)\n",
    "    \n",
    "        error = ySetTry - yPredTry\n",
    "        plot_error(NNInput, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Data ... \n",
      "\n",
      "    Loading Labeled Input from File: /home/venturi/WORKSPACE/SPES/spes//Data_PES/O3/Triat/PES_9//R.csv\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/venturi/WORKSPACE/SPES/spes//Data_PES/O3/Triat/PES_9//R.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c865135a8779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNNInput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainFlg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msgd_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNNInput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNNInput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-926a506aeafa>\u001b[0m in \u001b[0;36msgd_optimization\u001b[0;34m(NNInput)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasetsTry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNNInput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNNInput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/WORKSPACE/Spebus/PESConstruction/NN/TensorFlow_Keras/LoadData.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(NNInput)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m#################################################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mPathToLabeledInput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNInput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathToDataFldr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/R.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mRData\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mload_labeled_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPathToLabeledInput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mRDataOrig\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mRData\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/WORKSPACE/Spebus/PESConstruction/NN/TensorFlow_Keras/LoadData.py\u001b[0m in \u001b[0;36mload_labeled_input\u001b[0;34m(PathToLabeledInput)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_labeled_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPathToLabeledInput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    Loading Labeled Input from File: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mPathToLabeledInput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mxxData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPathToLabeledInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m#xDatay = pandas.read_csv(PathToData, header=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mxxData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxxData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/venturi/WORKSPACE/SPES/spes//Data_PES/O3/Triat/PES_9//R.csv'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    if not os.path.exists(NNInput.PathToOutputFldr):\n",
    "        os.makedirs(NNInput.PathToOutputFldr)\n",
    "\n",
    "    if (NNInput.TrainFlg):\n",
    "        sgd_optimization(NNInput)\n",
    "    else:\n",
    "        evaluate_model(NNInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
